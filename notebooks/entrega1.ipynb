{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Analysis - Customer Segmentation\n",
    "\n",
    "This notebook performs comprehensive clustering analysis using multiple algorithms:\n",
    "- K-Means\n",
    "- Hierarchical Clustering\n",
    "- DBSCAN\n",
    "- Gaussian Mixture Models (GMM)\n",
    "\n",
    "## Workflow:\n",
    "1. Data Loading\n",
    "2. Exploratory Data Analysis (EDA)\n",
    "3. Data Preprocessing (One-Hot Encoding + Scaling)\n",
    "4. Model Training & Evaluation\n",
    "5. Model Comparison\n",
    "6. Cluster Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Import custom utilities\n",
    "from utils.entrega1.data_loader import load_data\n",
    "from utils.entrega1.eda import check_missing_values_viz, plot_distributions_numerical\n",
    "from utils.entrega1.preprocessing import preprocess_pipeline\n",
    "from utils.entrega1.modeling import (\n",
    "    evaluate_clusters_kmeans,\n",
    "    plot_knn_distance,\n",
    "    optimize_dbscan_grid,\n",
    "    evaluate_gmm_bic,\n",
    "    compare_all_models_silhouette,\n",
    "    visualize_clusters_pca,\n",
    "    interpret_clusters\n",
    ")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = load_data('../data/datos_caso_1.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "check_missing_values_viz(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical distributions\n",
    "eda_numerical_cols = ['Year_Birth', 'Income', 'Recency', 'Kidhome', 'Teenhome', 'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth', 'Z_CostContact', 'Z_Revenue', 'Response']\n",
    "plot_distributions_numerical(df, eda_numerical_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing\n",
    "\n",
    "The preprocessing pipeline:\n",
    "1. Handles missing values\n",
    "2. Removes atypical values\n",
    "3. Creates engineered features (Age, Tenure, Total Purchases, etc.)\n",
    "4. **One-Hot Encodes** categorical features (Education, Marital_Status)\n",
    "5. **Scales** numerical features using StandardScaler\n",
    "\n",
    "**Important:** The pipeline returns transformers and feature lists for inverse transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data - returns scaled data, transformers, and feature lists\n",
    "df_scaled, scaler, encoder, all_feature_cols, num_feature_cols, ohe_feature_cols, categorical_cols = preprocess_pipeline(df)\n",
    "\n",
    "print(f\"Scaled data shape: {df_scaled.shape}\")\n",
    "print(f\"Total features: {len(all_feature_cols)}\")\n",
    "print(f\"Numerical features: {len(num_feature_cols)}\")\n",
    "print(f\"OHE features: {len(ohe_feature_cols)}\")\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. K-Means Clustering\n",
    "\n",
    "### 5.1 Determine Optimal K (Elbow + Silhouette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate K-Means for k=2 to k=19 with reference line at k=3\n",
    "evaluate_clusters_kmeans(df_scaled, range(2, 20), include_silhouette=True, ref_cluster=3, n_init=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Fit Final K-Means Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on elbow/silhouette analysis, choose optimal k (e.g., 6)\n",
    "optimal_k = 6\n",
    "\n",
    "kmeans_final = KMeans(n_clusters=optimal_k, n_init=20, random_state=123)\n",
    "kmeans_final.fit(df_scaled)\n",
    "\n",
    "print(f\"K-Means with k={optimal_k} fitted successfully\")\n",
    "print(f\"Inertia: {kmeans_final.inertia_:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display cluster centers (scaled values)\n",
    "print(\"Cluster Centers (Scaled):\")\n",
    "centers_scaled = pd.DataFrame(kmeans_final.cluster_centers_, columns=all_feature_cols)\n",
    "centers_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Visualize K-Means Clusters (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_kmeans = kmeans_final.labels_\n",
    "visualize_clusters_pca(df_scaled, labels_kmeans, title='K-Means Clusters (PCA)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit hierarchical clustering with same number of clusters as K-Means\n",
    "h_cluster_final = AgglomerativeClustering(n_clusters=optimal_k)\n",
    "labels_h_clust = h_cluster_final.fit_predict(df_scaled)\n",
    "\n",
    "print(f\"Hierarchical Clustering with {optimal_k} clusters fitted\")\n",
    "print(f\"Cluster distribution: {np.bincount(labels_h_clust)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize hierarchical clusters\n",
    "visualize_clusters_pca(df_scaled, labels_h_clust, title='Hierarchical Clusters (PCA)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. DBSCAN Clustering\n",
    "\n",
    "### 7.1 K-NN Distance Plot (Estimate Epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot k-nearest neighbors distance\n",
    "plot_knn_distance(df_scaled, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Grid Search for Optimal Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for epsilon and min_samples\n",
    "eps_values = np.arange(1.25, 1.50, 0.05)\n",
    "min_samples_values = np.arange(2, 10)\n",
    "\n",
    "print(\"Optimizing DBSCAN hyperparameters...\")\n",
    "dbscan_results = optimize_dbscan_grid(df_scaled, eps_values, min_samples_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View best parameters\n",
    "best_idx = dbscan_results['Score'].idxmax()\n",
    "best_params = dbscan_results.loc[best_idx]\n",
    "print(f\"Best DBSCAN parameters:\")\n",
    "print(f\"  Epsilon: {best_params['Epsilon']:.2f}\")\n",
    "print(f\"  Vecindad (min_samples): {int(best_params['Vecindad'])}\")\n",
    "print(f\"  Silhouette Score: {best_params['Score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Fit Final DBSCAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit DBSCAN with optimal parameters\n",
    "dbscan_final = DBSCAN(eps=best_params['Epsilon'], min_samples=int(best_params['Vecindad']))\n",
    "labels_dbscan = dbscan_final.fit_predict(df_scaled)\n",
    "\n",
    "print(f\"DBSCAN fitted\")\n",
    "print(f\"Unique clusters: {np.unique(labels_dbscan)}\")\n",
    "print(f\"Cluster distribution: {np.bincount(labels_dbscan[labels_dbscan >= 0])}\")\n",
    "print(f\"Noise points: {np.sum(labels_dbscan == -1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize DBSCAN clusters\n",
    "visualize_clusters_pca(df_scaled, labels_dbscan, title='DBSCAN Clusters (PCA)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Gaussian Mixture Model (GMM)\n",
    "\n",
    "### 8.1 BIC Analysis for Optimal Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate GMM using BIC across different covariance types\n",
    "evaluate_gmm_bic(df_scaled, range(2, 30), covariance_types=['spherical', 'tied', 'diag', 'full'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Fit Final GMM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on BIC plot, choose optimal configuration\n",
    "gmm_final = GaussianMixture(n_components=20, covariance_type='diag', random_state=123)\n",
    "gmm_final.fit(df_scaled)\n",
    "labels_gmm = gmm_final.predict(df_scaled)\n",
    "\n",
    "print(f\"GMM with 20 components (diag covariance) fitted\")\n",
    "print(f\"BIC: {gmm_final.bic(df_scaled):.2f}\")\n",
    "print(f\"AIC: {gmm_final.aic(df_scaled):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize GMM clusters\n",
    "visualize_clusters_pca(df_scaled, labels_gmm, title='GMM Clusters (PCA)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Comparison\n",
    "\n",
    "### 9.1 Silhouette Score Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models using Silhouette score\n",
    "labels_dict = {\n",
    "    'KMeans': labels_kmeans,\n",
    "    'Hierarchical': labels_h_clust,\n",
    "    'DBSCAN': labels_dbscan,\n",
    "    'GMM': labels_gmm\n",
    "}\n",
    "\n",
    "scores = compare_all_models_silhouette(df_scaled, labels_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Select Best Model\n",
    "\n",
    "Based on the Silhouette scores above, select the best performing model for interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model (example: KMeans)\n",
    "best_model_name = 'KMeans'\n",
    "best_labels = labels_kmeans\n",
    "\n",
    "print(f\"Selected model: {best_model_name}\")\n",
    "print(f\"Number of clusters: {len(np.unique(best_labels))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Cluster Interpretation\n",
    "\n",
    "### 10.1 Inverse Transform Cluster Centers\n",
    "\n",
    "Convert scaled cluster centers back to original units for interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cluster centers (scaled) - use all_feature_cols\n",
    "centers_scaled = pd.DataFrame(kmeans_final.cluster_centers_, columns=all_feature_cols)\n",
    "\n",
    "print(f\"Cluster centers shape: {centers_scaled.shape}\")\n",
    "print(f\"Numerical columns: {len(num_feature_cols)}\")\n",
    "print(f\"OHE columns: {len(ohe_feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse transform numerical features (scaler was fitted only on these)\n",
    "centers_num_inverse = scaler.inverse_transform(centers_scaled[num_feature_cols])\n",
    "\n",
    "# Inverse transform categorical features\n",
    "centers_cat_inverse = encoder.inverse_transform(centers_scaled[ohe_feature_cols])\n",
    "\n",
    "# Combine into final dataframe\n",
    "transformed_centers = pd.DataFrame(\n",
    "    np.concatenate([centers_num_inverse, centers_cat_inverse], axis=1),\n",
    "    columns=num_feature_cols + categorical_cols\n",
    ")\n",
    "\n",
    "print(\"\\nCluster Centers (Original Scale):\")\n",
    "transformed_centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Cluster Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cluster labels to scaled data\n",
    "df_with_clusters = df_scaled.copy()\n",
    "df_with_clusters['Cluster'] = best_labels\n",
    "\n",
    "# Cluster size distribution\n",
    "cluster_sizes = df_with_clusters['Cluster'].value_counts().sort_index()\n",
    "print(\"Cluster Sizes:\")\n",
    "print(cluster_sizes)\n",
    "\n",
    "# Visualize cluster sizes\n",
    "plt.figure(figsize=(10, 6))\n",
    "cluster_sizes.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "plt.title('Cluster Size Distribution')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3 Cluster Profiles\n",
    "\n",
    "Interpret each cluster based on the transformed centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display key characteristics for each cluster\n",
    "key_features = ['Age', 'Income', 'Total_Mnt', 'Total_Num_Purchases', 'Recency', 'Education', 'Marital_Status']\n",
    "available_features = [f for f in key_features if f in transformed_centers.columns]\n",
    "\n",
    "print(\"\\nKey Cluster Characteristics:\")\n",
    "transformed_centers[available_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusions\n",
    "\n",
    "Summary of findings:\n",
    "1. **Best Model**: [Based on Silhouette scores]\n",
    "2. **Number of Clusters**: [Optimal k]\n",
    "3. **Key Insights**: [Describe main customer segments]\n",
    "\n",
    "### Next Steps:\n",
    "- Deploy segmentation model\n",
    "- Create targeted marketing strategies for each segment\n",
    "- Monitor cluster stability over time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
